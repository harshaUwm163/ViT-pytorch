{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import valid, set_seed\n",
    "from apex import amp\n",
    "import torch\n",
    "import argparse\n",
    "from addict import Dict\n",
    "import logging\n",
    "from utils.construct_tff import construct_real_tff\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from utils.data_utils import get_loader\n",
    "from models.modeling import VisionTransformer, CONFIGS\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "!export CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return params/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Dict()\n",
    "args.model_type = 'ViT-B_16'\n",
    "args.dataset = 'inet1k_cats'\n",
    "args.img_size = 224\n",
    "args.pretrained_dir = 'checkpoint/ViT-B_16-224.npz'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.device = device\n",
    "args.local_rank = -1\n",
    "args.train_batch_size = 16\n",
    "args.eval_batch_size = 16\n",
    "\n",
    "# args.dataset = 'inet1k_cats'\n",
    "# args.dataset_dir = 'data/inet1k_classes/cats'\n",
    "# ckpt_path = 'output/inet1k_cats-2023-10-02-22-19-15/inet1k_cats_final_ckpt.bin' \n",
    "args.dataset = 'inet1k_birds'\n",
    "args.dataset_dir = 'data/inet1k_classes/birds'\n",
    "ckpt_path = 'output/inet1k_birds-2023-10-02-22-25-22/inet1k_birds_final_ckpt.bin'\n",
    "# args.dataset = 'inet1k_dogs'\n",
    "# args.dataset_dir = 'data/inet1k_classes/dogs'\n",
    "# ckpt_path = 'output/inet1k_dogs-2023-09-24-21-00-17/inet1k_dogs_final_ckpt.bin'\n",
    "# args.dataset = 'inet1k_snakes'\n",
    "# args.dataset_dir = 'data/inet1k_classes/snakes'\n",
    "# ckpt_path = 'output/inet1k_snakes-2023-10-02-22-28-06/inet1k_snakes_final_ckpt.bin'\n",
    "# args.dataset = 'inet1k_trucks'\n",
    "# args.dataset_dir = 'data/inet1k_classes/trucks'\n",
    "# ckpt_path = 'output/inet1k_trucks-2023-09-24-20-47-28/inet1k_trucks_final_ckpt.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 02:23:29,844 - __main__ - INFO - classifier: token\n",
      "hidden_size: 768\n",
      "patches:\n",
      "  size: !!python/tuple\n",
      "  - 16\n",
      "  - 16\n",
      "representation_size: null\n",
      "transformer:\n",
      "  attention_dropout_rate: 0.0\n",
      "  dropout_rate: 0.1\n",
      "  mlp_dim: 3072\n",
      "  num_heads: 12\n",
      "  num_layers: 12\n",
      "\n",
      "2023-10-17 02:23:29,845 - __main__ - INFO - Training parameters {'model_type': 'ViT-B_16', 'dataset': 'inet1k_birds', 'img_size': 224, 'pretrained_dir': 'checkpoint/ViT-B_16-224.npz', 'device': device(type='cuda'), 'local_rank': -1, 'train_batch_size': 16, 'eval_batch_size': 16, 'dataset_dir': 'data/inet1k_classes/birds'}\n",
      "2023-10-17 02:23:29,845 - __main__ - INFO - Total Parameter: \t86.6M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.567656\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "config = CONFIGS[args.model_type]\n",
    "\n",
    "if args.dataset == \"cifar10\":\n",
    "    num_classes = 10\n",
    "elif args.dataset == \"cifar100\":\n",
    "    num_classes = 100\n",
    "elif 'inet' in args.dataset:\n",
    "    num_classes = 1000\n",
    "\n",
    "model = VisionTransformer(config, args.img_size, zero_head=False, num_classes=num_classes)\n",
    "model.load_from(np.load(args.pretrained_dir))\n",
    "model.to(args.device)\n",
    "num_params = count_parameters(model)\n",
    "\n",
    "logger.info(\"{}\".format(config))\n",
    "logger.info(\"Training parameters %s\", args)\n",
    "logger.info(\"Total Parameter: \\t%2.1fM\" % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_loader(args)\n",
    "classes = train_loader.dataset.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3387,  0.5969, -0.5674,  ..., -0.3600,  0.3715, -0.2212],\n",
      "        [ 0.6837,  0.0188,  0.0290,  ..., -0.1652,  0.0880, -0.1276],\n",
      "        [ 0.8962, -0.6325,  0.3972,  ...,  0.4540, -0.1371, -0.3120],\n",
      "        ...,\n",
      "        [ 0.0443, -0.0582, -0.0945,  ...,  0.1240,  0.0954, -0.1654],\n",
      "        [ 0.5564,  0.5776,  0.4610,  ...,  0.1824,  0.0287, -1.5507],\n",
      "        [ 0.5302, -0.3743, -0.5047,  ..., -0.5446, -0.4084, -0.1608]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data/harsha/ViT-pytorch/classifier_select_Vit_ft.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bolvi1/data/harsha/ViT-pytorch/classifier_select_Vit_ft.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m out_logits, _ \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bolvi1/data/harsha/ViT-pytorch/classifier_select_Vit_ft.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(out_logits)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bolvi1/data/harsha/ViT-pytorch/classifier_select_Vit_ft.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(classes[label])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bolvi1/data/harsha/ViT-pytorch/classifier_select_Vit_ft.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "for data, label in train_loader:\n",
    "    data = data.to(device)\n",
    "    out_logits, _ = model(data)\n",
    "    print(out_logits)\n",
    "    print(classes[label])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 lorikeet\n",
      "12 bald_eagle\n",
      "15 prairie_chicken\n",
      "13 vulture\n",
      "13 African_grey\n",
      "13 African_grey\n",
      "13 lorikeet\n",
      "14 hummingbird\n",
      "12 vulture\n",
      "7 bee_eater\n",
      "15 coucal\n",
      "13 lorikeet\n",
      "11 bald_eagle\n",
      "13 lorikeet\n",
      "15 great_grey_owl\n",
      "14 bald_eagle\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(torch.amax(out_logits, dim=1).int(), label.detach()):\n",
    "    print(i.item(), classes[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1000])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnetff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
